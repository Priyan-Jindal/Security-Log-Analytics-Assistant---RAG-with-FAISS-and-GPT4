{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928ade5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up GCS client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = \"arize_rag\"  # Change to your actual bucket name\n",
    "folder_name = \"netflow_data/\"  # Folder inside the bucket where CSVs are stored\n",
    "output_faiss_dir = \"faiss_network_logs\"\n",
    "OUTPUT_FILE = \"processed_network_logs.json\"\n",
    "\n",
    "# Define fields we care about\n",
    "COLUMNS = [\"Time\", \"Duration\", \"SrcDevice\", \"DstDevice\", \"Protocol\", \"SrcPort\", \"DstPort\", \n",
    "           \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"]\n",
    "\n",
    "# List all CSV files in GCS\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "csv_file = \"netflow_data/netflow_day-02.csv\"  # Only process this file\n",
    "blob = bucket.blob(csv_file)\n",
    "\n",
    "print(f\"ðŸ“¥ Downloading {csv_file} from GCS...\")\n",
    "csv_content = blob.download_as_text()\n",
    "\n",
    "# Load only the first 100K rows (instead of all rows)\n",
    "df = pd.read_csv(StringIO(csv_content), names=COLUMNS, nrows=100000)\n",
    "print(f\"Loaded DataFrame with {len(df)} rows (limited to 100K)\")\n",
    "\n",
    "# Convert each row into a structured JSON entry\n",
    "all_logs = []\n",
    "for _, row in df.iterrows():\n",
    "    log_text = (\n",
    "        f\"At time {row['Time']}, device {row['SrcDevice']} initiated a \"\n",
    "        f\"{'TCP' if row['Protocol'] == 6 else 'UDP' if row['Protocol'] == 17 else 'other'} connection \"\n",
    "        f\"to {row['DstDevice']} on port {row['DstPort']}. \"\n",
    "        f\"The source used port {row['SrcPort']} and sent {row['SrcPackets']} packets \"\n",
    "        f\"({row['SrcBytes']} bytes), while the destination responded with {row['DstPackets']} packets \"\n",
    "        f\"({row['DstBytes']} bytes).\"\n",
    "    )\n",
    "    all_logs.append({\"text\": log_text, \"metadata\": row.to_dict()})\n",
    "\n",
    "# Save processed logs to JSON\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(all_logs, f, indent=4)\n",
    "\n",
    "print(f\"Processed {len(all_logs)} log entries and saved to {OUTPUT_FILE}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb4a99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "with open(OUTPUT_FILE, \"r\") as f:\n",
    "    log_entries = json.load(f)\n",
    "    \n",
    "# MAKE SURE U GET UR GPT KEY HERE\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# Normalize embeddings (important for cosine similarity search)\n",
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms\n",
    "\n",
    "# Convert text logs to embeddings\n",
    "texts = [entry[\"text\"] for entry in log_entries]\n",
    "raw_embeddings = embeddings.embed_documents(texts)\n",
    "normalized_embeddings = normalize_embeddings(np.array(raw_embeddings))\n",
    "\n",
    "# Define optimized FAISS index (HNSW for large-scale search)\n",
    "d = len(normalized_embeddings[0])  # Dimensionality\n",
    "index = faiss.IndexHNSWFlat(d, 32)  # 32 neighbors\n",
    "\n",
    "# Tune HNSW parameters for speed vs. accuracy\n",
    "index.hnsw.efSearch = 64  # Higher = more accurate, lower = faster\n",
    "index.hnsw.efConstruction = 200  # Balances recall vs. indexing time\n",
    "\n",
    "# Add embeddings to FAISS index\n",
    "index.add(normalized_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"faiss_index\")\n",
    "\n",
    "print(\"FAISS index saved locally. Uploading to GCS...\")\n",
    "\n",
    "# Upload FAISS index to GCS\n",
    "os.system(f\"gsutil cp faiss_index gs://{bucket_name}/\")\n",
    "\n",
    "print(\"FAISS index uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede53d6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"faiss_index\")\n",
    "\n",
    "# Print FAISS info\n",
    "print(f\"FAISS Index Loaded - {index.ntotal} vectors stored\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
